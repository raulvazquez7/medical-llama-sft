{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a14143",
   "metadata": {},
   "source": [
    "# Medical LLM Fine-Tuning with QLoRA\n",
    " \n",
    "This notebook demonstrates how to fine-tune Llama 3.1-8B-Instruct for \n",
    "medical reasoning using QLoRA (Quantized Low-Rank Adaptation).\n",
    "Inspired by: HuatuoGPT-o1 (https://github.com/FreedomIntelligence/HuatuoGPT-o1)\n",
    "\n",
    "Dataset: FreedomIntelligence/medical-o1-reasoning-SFT\n",
    "Model: meta-llama/Llama-3.1-8B-Instruct\n",
    "\n",
    "Hardware: Optimized for Google Colab Free (T4 GPU, ~15GB VRAM)\n",
    "Training time: ~2-3 hours on Colab Free\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c57c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Install required packages with compatible versions\n",
    "!pip install -q \\\n",
    "    torch \\\n",
    "    transformers>=4.44.0 \\\n",
    "    datasets>=2.20.0 \\\n",
    "    accelerate>=0.34.0 \\\n",
    "    peft>=0.12.0 \\\n",
    "    trl>=0.9.0 \\\n",
    "    bitsandbytes>=0.44.0 \\\n",
    "    sentencepiece\n",
    "\n",
    "# Reiniciar el kernel\n",
    "#import IPython\n",
    "#IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9w38fvw9aw4",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 2. HUGGINGFACE AUTHENTICATION (REQUIRED)\n",
    "# ============================================================================\n",
    "\n",
    "**IMPORTANT**: This notebook uses Llama 3.1-8B-Instruct, which requires:\n",
    "1. HuggingFace account\n",
    "2. Approval to access Meta-Llama models\n",
    "3. Valid access token\n",
    "\n",
    "## Setup Instructions:\n",
    "\n",
    "### Step 1: Request Llama Access (One-time)\n",
    "1. Visit: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "2. Click **\"Request Access\"** and fill out Meta's form\n",
    "3. Wait for approval email (typically 1-24 hours)\n",
    "\n",
    "### Step 2: Create Access Token\n",
    "1. Go to: https://huggingface.co/settings/tokens\n",
    "2. Click **\"New token\"** ‚Üí Name it (e.g., \"medical-llm\") ‚Üí Select **\"Read\"** permission\n",
    "3. Copy the token (starts with `hf_...`)\n",
    "\n",
    "### Step 3: Run the authentication cell below ‚¨áÔ∏è\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49oek4mvklo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2.1 AUTHENTICATE WITH HUGGINGFACE\n",
    "# ============================================================================\n",
    "\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Check if already authenticated\n",
    "try:\n",
    "    from huggingface_hub import HfFolder\n",
    "    token = HfFolder.get_token()\n",
    "    if token:\n",
    "        print(\"‚úÖ Already authenticated with HuggingFace!\")\n",
    "        print(f\"Token found: {token[:10]}...\" if token else \"No token\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No token found. Please authenticate below.\")\n",
    "        raise ValueError(\"Authentication required\")\n",
    "except:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üîê HUGGINGFACE AUTHENTICATION REQUIRED\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nThis will open an interactive prompt to enter your HuggingFace token.\")\n",
    "    print(\"Paste your token (starts with 'hf_...') and press Enter.\\n\")\n",
    "    print(\"‚ö†Ô∏è If you don't have a token yet, follow the instructions above ‚¨ÜÔ∏è\\n\")\n",
    "    \n",
    "    # Interactive login (will prompt for token)\n",
    "    login()\n",
    "    \n",
    "    print(\"\\n‚úÖ Authentication successful!\")\n",
    "    print(\"You can now download Llama 3.1-8B-Instruct\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Verify access to Llama model\n",
    "print(\"\\nüîç Verifying access to meta-llama/Meta-Llama-3.1-8B-Instruct...\")\n",
    "try:\n",
    "    from huggingface_hub import model_info\n",
    "    info = model_info(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "    print(\"‚úÖ Access verified! You can proceed with the notebook.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå ERROR: Cannot access Llama 3.1 model\")\n",
    "    print(f\"Error: {str(e)}\\n\")\n",
    "    print(\"Possible issues:\")\n",
    "    print(\"  1. You haven't requested access to Meta-Llama models yet\")\n",
    "    print(\"     ‚Üí Visit: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "    print(\"  2. Your access request is still pending approval\")\n",
    "    print(\"     ‚Üí Wait for approval email from Meta (1-24 hours)\")\n",
    "    print(\"  3. Invalid token\")\n",
    "    print(\"     ‚Üí Generate a new token at: https://huggingface.co/settings/tokens\")\n",
    "    print(\"\\n‚ö†Ô∏è DO NOT PROCEED until this is resolved\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893e5cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset, Dataset\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check GPU availability\n",
    "# QLoRA requires CUDA for 4-bit quantization\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Total VRAM: {gpu_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: No GPU detected. Training will be extremely slow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yq2g7mzcdsm",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 3. DATA UPLOAD (REQUIRED)\n",
    "# ============================================================================\n",
    "\n",
    "**IMPORTANT**: You need to upload the training data files to Colab.\n",
    "\n",
    "## Option 1: Upload from Local Machine\n",
    "\n",
    "If you have cloned the repository and have the data files:\n",
    "\n",
    "1. Click the **folder icon** üìÅ in the left sidebar\n",
    "2. Click the **upload icon** ‚¨ÜÔ∏è \n",
    "3. Upload both files:\n",
    "   - `train_data.jsonl` (~1.3 MB, ~450 examples)\n",
    "   - `test_data.jsonl` (~145 KB, ~50 examples)\n",
    "\n",
    "**The cell below will verify that files are uploaded before proceeding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class TrainingConfig:\n",
    "    \"\"\"\n",
    "    Centralized configuration for reproducibility and easy experimentation.\n",
    "    \n",
    "    These hyperparameters are optimized for:\n",
    "    - Google Colab Free (T4 GPU, ~15GB VRAM)\n",
    "    - ~450 training examples\n",
    "    - Balance between quality and training time\n",
    "    \"\"\"\n",
    "    \n",
    "    # Model Configuration\n",
    "    MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "    \n",
    "    # QLoRA Configuration\n",
    "    # Using rank=16 provides good balance between parameter efficiency and model capacity\n",
    "    LORA_R = 16\n",
    "    # Alpha is typically set to 2*rank for stable training\n",
    "    LORA_ALPHA = 32\n",
    "    # We target attention layers as they're most impactful for adapting LLM behavior\n",
    "    LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "    LORA_DROPOUT = 0.05\n",
    "    \n",
    "    # Quantization Configuration\n",
    "    # NF4 (4-bit NormalFloat) is optimal for LLMs as it preserves important weight distributions\n",
    "    QUANTIZATION_TYPE = \"nf4\"\n",
    "    # Double quantization further reduces memory with minimal quality loss\n",
    "    USE_DOUBLE_QUANT = True\n",
    "    COMPUTE_DTYPE = torch.bfloat16  # bfloat16 is more stable than float16 for training\n",
    "    \n",
    "    # Training Hyperparameters\n",
    "    # Small batch size due to memory constraints; gradient accumulation compensates\n",
    "    PER_DEVICE_BATCH_SIZE = 2\n",
    "    # Effective batch size = 2 * 4 = 8\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "    # 2e-4 is standard for LoRA; lower than full fine-tuning to avoid catastrophic forgetting\n",
    "    LEARNING_RATE = 2e-4\n",
    "    # More epochs on small dataset helps model learn the reasoning pattern\n",
    "    NUM_EPOCHS = 3\n",
    "    # Warmup prevents early training instability\n",
    "    WARMUP_RATIO = 0.03\n",
    "    # Longer sequences needed for complex medical reasoning chains\n",
    "    MAX_SEQ_LENGTH = 2048\n",
    "    \n",
    "    # Optimization\n",
    "    # AdamW is standard; paged optimizers reduce memory fragmentation\n",
    "    OPTIMIZER_TYPE = \"paged_adamw_32bit\"\n",
    "    # Cosine decay smoothly reduces LR, helping convergence\n",
    "    LR_SCHEDULER_TYPE = \"cosine\"\n",
    "    \n",
    "    # Logging and Checkpointing\n",
    "    LOGGING_STEPS = 10\n",
    "    SAVE_STEPS = 50\n",
    "    EVAL_STEPS = 50\n",
    "    \n",
    "    # Output\n",
    "    OUTPUT_DIR = \"./medical-llm-finetuned\"\n",
    "    \n",
    "    # Reproducibility\n",
    "    SEED = 42\n",
    "\n",
    "config = TrainingConfig()\n",
    "print(\"‚úì Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. DATA LOADING AND PREPARATION\n",
    "# ============================================================================\n",
    "\n",
    "# Upload your train_data.jsonl and test_data.jsonl files using the file upload button\n",
    "# or drag and drop them into Colab's file browser\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "def load_local_jsonl(filepath):\n",
    "    \"\"\"Load JSONL file into list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Check if data files exist, otherwise prompt upload\n",
    "if not os.path.exists('train_data.jsonl') or not os.path.exists('test_data.jsonl'):\n",
    "    print(\"Please upload train_data.jsonl and test_data.jsonl\")\n",
    "    uploaded = files.upload()\n",
    "else:\n",
    "    print(\"‚úì Data files found\")\n",
    "\n",
    "# Load datasets\n",
    "train_data = load_local_jsonl('train_data.jsonl')\n",
    "test_data = load_local_jsonl('test_data.jsonl')\n",
    "\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "print(f\"Test examples: {len(test_data)}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE TRAINING EXAMPLE\")\n",
    "print(\"=\"*80)\n",
    "sample = train_data[0]\n",
    "print(f\"Question: {sample['question'][:200]}...\")\n",
    "print(f\"\\nComplex CoT: {sample['complex_cot'][:300]}...\")\n",
    "print(f\"\\nResponse: {sample['response'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f2006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. PROMPT FORMATTING\n",
    "# ============================================================================\n",
    "\n",
    "def format_medical_prompt(example):\n",
    "    \"\"\"\n",
    "    Format training example into Llama 3.1 chat template with HuatuoGPT-o1 style.\n",
    "    \n",
    "    The model learns to:\n",
    "    1. First generate step-by-step reasoning (## Thinking)\n",
    "    2. Then provide a concise final answer (## Final Response)\n",
    "    \n",
    "    This two-stage format improves reasoning quality and interpretability.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are a medical expert AI assistant. When answering medical questions, \"\n",
    "        \"first provide your step-by-step reasoning in a '## Thinking' section, \"\n",
    "        \"then provide your final answer in a '## Final Response' section.\"\n",
    "    )\n",
    "    \n",
    "    # Combine CoT and response in HuatuoGPT-o1 format\n",
    "    assistant_response = f\"\"\"## Thinking\n",
    "{example['complex_cot']}\n",
    "\n",
    "## Final Response\n",
    "{example['response']}\"\"\"\n",
    "    \n",
    "    # Llama 3.1 chat template format\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": example['question']},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_response}\n",
    "    ]\n",
    "    \n",
    "    return messages\n",
    "\n",
    "# Test formatting\n",
    "formatted_example = format_medical_prompt(train_data[0])\n",
    "print(\"‚úì Prompt formatting function ready\")\n",
    "print(f\"\\nFormatted message structure:\")\n",
    "for msg in formatted_example:\n",
    "    print(f\"  - {msg['role']}: {len(msg['content'])} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e04ee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. MODEL AND TOKENIZER LOADING\n",
    "# ============================================================================\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "# This reduces model memory footprint from ~16GB to ~4GB with minimal quality loss\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=config.QUANTIZATION_TYPE,\n",
    "    bnb_4bit_compute_dtype=config.COMPUTE_DTYPE,\n",
    "    bnb_4bit_use_double_quant=config.USE_DOUBLE_QUANT,\n",
    ")\n",
    "\n",
    "print(\"Loading base model (this may take a few minutes)...\")\n",
    "\n",
    "# Load model with quantization\n",
    "# device_map=\"auto\" automatically distributes model across available devices\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "\n",
    "# Llama models don't have a default pad token; we set it to EOS to avoid warnings\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "# This freezes base model weights and prepares LoRA adapter layers\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(\"‚úì Model and tokenizer loaded successfully\")\n",
    "print(f\"Model memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. LORA CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Configure LoRA adapters\n",
    "# LoRA adds trainable low-rank matrices to attention layers while keeping\n",
    "# the base model frozen, dramatically reducing trainable parameters\n",
    "lora_config = LoraConfig(\n",
    "    r=config.LORA_R,\n",
    "    lora_alpha=config.LORA_ALPHA,\n",
    "    target_modules=config.LORA_TARGET_MODULES,\n",
    "    lora_dropout=config.LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_percentage = 100 * trainable_params / total_params\n",
    "\n",
    "print(\"‚úì LoRA adapters configured\")\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({trainable_percentage:.2f}%)\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"\\nMemory savings: ~{100 - trainable_percentage:.1f}% of parameters frozen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b422b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. DATASET PREPARATION FOR TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def format_for_trainer(example):\n",
    "    \"\"\"\n",
    "    Format example for SFTTrainer using Llama's chat template.\n",
    "    \n",
    "    SFTTrainer expects a 'text' field with the fully formatted prompt.\n",
    "    We use apply_chat_template to ensure proper special token handling.\n",
    "    \"\"\"\n",
    "    messages = format_medical_prompt(example)\n",
    "    # apply_chat_template handles special tokens (<|begin_of_text|>, etc.)\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False  # We already have the assistant's response\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Convert to HuggingFace Dataset format\n",
    "# SFTTrainer works best with Dataset objects for built-in batching and shuffling\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "# Apply formatting\n",
    "train_dataset = train_dataset.map(format_for_trainer)\n",
    "test_dataset = test_dataset.map(format_for_trainer)\n",
    "\n",
    "print(\"‚úì Datasets prepared for training\")\n",
    "print(f\"Train dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} examples\")\n",
    "\n",
    "# Preview a formatted training example\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FORMATTED TRAINING EXAMPLE (first 500 chars)\")\n",
    "print(\"=\"*80)\n",
    "print(train_dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f98320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 8. TRAINING CONFIGURATION (ACTUALIZADO para trl reciente)\n",
    "# ============================================================================\n",
    "\n",
    "from trl import SFTConfig\n",
    "\n",
    "# SFTConfig reemplaza TrainingArguments y a√±ade par√°metros espec√≠ficos de SFT\n",
    "training_args = SFTConfig(\n",
    "    # Output\n",
    "    output_dir=config.OUTPUT_DIR,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=config.NUM_EPOCHS,\n",
    "    per_device_train_batch_size=config.PER_DEVICE_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    optim=config.OPTIMIZER_TYPE,\n",
    "    lr_scheduler_type=config.LR_SCHEDULER_TYPE,\n",
    "    warmup_ratio=config.WARMUP_RATIO,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    \n",
    "    # Logging and evaluation\n",
    "    logging_steps=config.LOGGING_STEPS,\n",
    "    eval_steps=config.EVAL_STEPS,\n",
    "    save_steps=config.SAVE_STEPS,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=config.SEED,\n",
    "    data_seed=config.SEED,\n",
    "    \n",
    "    # Performance\n",
    "    dataloader_num_workers=0,\n",
    "    group_by_length=False,\n",
    "    \n",
    "    # Reporting\n",
    "    report_to=\"none\",\n",
    "    run_name=\"medical-llm-sft\",\n",
    "    \n",
    "    # ============================================\n",
    "    # SFT-specific parameters (nuevo en SFTConfig)\n",
    "    # ============================================\n",
    "    max_length=config.MAX_SEQ_LENGTH,\n",
    "    packing=False,  # Disable for medical data to preserve context\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "print(\"‚úì Training configuration ready (SFTConfig)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f6b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 9. INITIALIZE TRAINER (SIMPLIFICADO)\n",
    "# ============================================================================\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Initialize SFTTrainer (ahora mucho m√°s simple)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,  # SFTConfig contiene todo\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized and ready for training\")\n",
    "print(f\"\\nEstimated training time on Colab T4:\")\n",
    "print(f\"  ~{len(train_dataset) * config.NUM_EPOCHS / (config.PER_DEVICE_BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS * 10):.1f} minutes per epoch\")\n",
    "print(f\"  Total: ~{len(train_dataset) * config.NUM_EPOCHS / (config.PER_DEVICE_BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS * 10) * config.NUM_EPOCHS:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb87618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 10. TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"This will take approximately 2-3 hours on Colab Free T4 GPU\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train the model\n",
    "# The trainer will automatically handle batching, gradient accumulation,\n",
    "# checkpointing, and evaluation\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì Training completed!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training took: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "\n",
    "# Save final metrics\n",
    "trainer.log_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_metrics(\"train\", train_result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec415204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 11. EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Evaluating fine-tuned model on test set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Save evaluation results\n",
    "trainer.save_metrics(\"eval\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 12. QUALITATIVE EVALUATION - GENERATE SAMPLE PREDICTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def generate_response(question, max_new_tokens=512):\n",
    "    \"\"\"\n",
    "    Generate a response for a medical question using the fine-tuned model.\n",
    "    \n",
    "    Returns both the thinking process and final response in HuatuoGPT-o1 format.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are a medical expert AI assistant. When answering medical questions, \"\n",
    "        \"first provide your step-by-step reasoning in a '## Thinking' section, \"\n",
    "        \"then provide your final answer in a '## Final Response' section.\"\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    # Format and tokenize\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True  # Add prompt for model to continue\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    # temperature=0.7 balances creativity and consistency\n",
    "    # top_p=0.9 uses nucleus sampling for more natural text\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    # Decode and extract only the new tokens (response)\n",
    "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test on a few examples from test set\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE PREDICTIONS FROM FINE-TUNED MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "num_samples = 3\n",
    "for i in range(min(num_samples, len(test_data))):\n",
    "    example = test_data[i]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EXAMPLE {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nQUESTION:\")\n",
    "    print(example['question'])\n",
    "    \n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(\"MODEL RESPONSE:\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    response = generate_response(example['question'])\n",
    "    print(response)\n",
    "    \n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(\"EXPECTED (Ground Truth):\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"## Thinking\\n{example['complex_cot'][:300]}...\")\n",
    "    print(f\"\\n## Final Response\\n{example['response'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 13. SAVE FINE-TUNED MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Save LoRA adapters\n",
    "# These are small (~100MB) and contain only the trained weights\n",
    "final_model_path = f\"{config.OUTPUT_DIR}/final_model\"\n",
    "model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"‚úì Model saved to: {final_model_path}\")\n",
    "print(f\"\\nSaved files:\")\n",
    "!ls -lh {final_model_path}\n",
    "\n",
    "# Optional: Create a zip file for easy download\n",
    "!zip -r medical_llm_adapters.zip {final_model_path}\n",
    "print(\"\\n‚úì Created medical_llm_adapters.zip for download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a1a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 14. DOWNLOAD MODEL (Optional)\n",
    "# ============================================================================\n",
    "\n",
    "# Download the model adapters to your local machine\n",
    "# Uncomment the line below to trigger download\n",
    "# files.download('medical_llm_adapters.zip')\n",
    "\n",
    "print(\"To download the model:\")\n",
    "print(\"1. Uncomment the line above, or\")\n",
    "print(\"2. Use the file browser on the left to download manually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921cfe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 15. NEXT STEPS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\"\"\n",
    "‚úÖ TRAINING COMPLETED SUCCESSFULLY!\n",
    "\n",
    "Next Steps:\n",
    "-----------\n",
    "1. Download the fine-tuned LoRA adapters (medical_llm_adapters.zip)\n",
    "   \n",
    "2. To use the model locally, load it with:\n",
    "   ```python\n",
    "   from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "   from peft import PeftModel\n",
    "   \n",
    "   base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "   model = PeftModel.from_pretrained(base_model, \"./path/to/adapters\")\n",
    "   tokenizer = AutoTokenizer.from_pretrained(\"./path/to/adapters\")\n",
    "   ```\n",
    "\n",
    "3. Optional: Push to HuggingFace Hub for easy sharing\n",
    "   - Create account at huggingface.co\n",
    "   - Run: model.push_to_hub(\"your-username/medical-llm-finetuned\")\n",
    "\n",
    "4. Run more comprehensive evaluation on medical benchmarks\n",
    "\n",
    "5. Compare with baseline Llama 3.1-8B-Instruct to quantify improvement\n",
    "\n",
    "Resources:\n",
    "----------\n",
    "- HuatuoGPT-o1 Paper: https://arxiv.org/abs/2412.18925\n",
    "- QLoRA Paper: https://arxiv.org/abs/2305.14314\n",
    "- TRL Documentation: https://huggingface.co/docs/trl\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
